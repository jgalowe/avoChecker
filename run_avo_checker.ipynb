{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce2d0150",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnl\\AppData\\Local\\Temp\\ipykernel_19940\\3686910131.py:6: DeprecationWarning: 'imghdr' is deprecated and slated for removal in Python 3.13\n",
      "  import imghdr\n",
      "c:\\Users\\johnl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 16 variables whereas the saved optimizer has 30 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "#load libraries and the classification model\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import cv2\n",
    "import imghdr\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import shutil\n",
    "from PIL import Image\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('avo_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f321982",
   "metadata": {},
   "source": [
    "First we run the yolo model to identify the kind of avo. Since we can only predict Hass, maybe I need something to account for this. \n",
    "\n",
    "Next we move these images into a separate checking folder. This is to make the rest of the code sustainable and work across all devices consistently. This will move all the Hass crops into a separate checking folder (or shepards at a later date, may need an if else or something) and then delete the runs folder to avoid having the future cops go into 'predict2' and then having to keep track of the number of predicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c673464b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.91 ğŸš€ Python-3.12.9 torch-2.6.0+cpu CPU (AMD Ryzen 7 3700X 8-Core Processor)\n",
      "YOLO11s summary (fused): 100 layers, 9,413,574 parameters, 0 gradients, 21.3 GFLOPs\n",
      "\n",
      "image 1/7 c:\\Users\\johnl\\Documents\\GitHub\\avoChecker\\Pictures\\1020\\482736535_936074375403619_6568392030990003422_n.jpg: 640x512 1 Hass, 121.3ms\n",
      "image 2/7 c:\\Users\\johnl\\Documents\\GitHub\\avoChecker\\Pictures\\1020\\483134351_9435740509878282_1840051949765373307_n.jpg: 640x512 1 Hass, 99.3ms\n",
      "image 3/7 c:\\Users\\johnl\\Documents\\GitHub\\avoChecker\\Pictures\\1020\\488607736_3071586109682685_3680888725088702396_n.jpg: 640x512 1 Hass, 98.7ms\n",
      "image 4/7 c:\\Users\\johnl\\Documents\\GitHub\\avoChecker\\Pictures\\1020\\488850548_969574118298263_4386745660247402047_n.jpg: 640x512 2 Hasss, 99.6ms\n",
      "image 5/7 c:\\Users\\johnl\\Documents\\GitHub\\avoChecker\\Pictures\\1020\\488921841_585538767871724_5559439902032879049_n.jpg: 640x512 2 Hasss, 97.8ms\n",
      "image 6/7 c:\\Users\\johnl\\Documents\\GitHub\\avoChecker\\Pictures\\1020\\490996281_9470889019696806_6646285696310706314_n.jpg: 640x512 1 Hass, 99.3ms\n",
      "image 7/7 c:\\Users\\johnl\\Documents\\GitHub\\avoChecker\\Pictures\\1020\\490998978_506788389035584_8423710632793732114_n.jpg: 640x512 1 Hass, 101.5ms\n",
      "Speed: 2.4ms preprocess, 102.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Results saved to \u001b[1mruns\\detect\\predict\u001b[0m\n",
      "ğŸ’¡ Learn more at https://docs.ultralytics.com/modes/predict\n"
     ]
    }
   ],
   "source": [
    "!yolo predict model=avoID.pt source='Pictures/1020' conf = 0.5 save_crop=True save\n",
    "shutil.move('runs/detect/predict/crops/Hass','checking_folder/') #need to add a line if there are no hass' detected\n",
    "shutil.rmtree('runs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77550121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.4497638"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is to resize the image prior to running it through the model\n",
    "def prepare_input(image_path, target_size=(256, 256)): #adjust target size to match model input.\n",
    "    img = Image.open(image_path).resize(target_size)\n",
    "    img_array = np.array(img) / 255.0  # Normalize pixel values\n",
    "    img_array = np.expand_dims(img_array, axis=0) # Add batch dimension\n",
    "    return img_array\n",
    "\n",
    "avo_pics = os.listdir('checking_folder/')\n",
    "\n",
    "avo_score = [] #empty list to store the avo score\n",
    "\n",
    "#Loop through all the pics in the folder to get the individual predictions for each avo\n",
    "for pic in avo_pics:\n",
    "    avo = prepare_input('checking_folder/' + pic)\n",
    "    avo_score.append(model.predict(avo))\n",
    "\n",
    "shutil.rmtree('checking_folder/') #delete the checking_folder to ensure we can use exactly the same code in the futur\n",
    "\n",
    "avo_score = np.concatenate(avo_score, axis=0 )\n",
    "avo_score.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

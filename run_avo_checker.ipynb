{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce2d0150",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnl\\AppData\\Local\\Temp\\ipykernel_11672\\1336455791.py:6: DeprecationWarning: 'imghdr' is deprecated and slated for removal in Python 3.13\n",
      "  import imghdr\n"
     ]
    }
   ],
   "source": [
    "#load libraries and the classification model\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import cv2\n",
    "import imghdr\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import shutil\n",
    "from PIL import Image\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('avo_model.keras')\n",
    "from ultralytics import YOLO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f321982",
   "metadata": {},
   "source": [
    "First we run the yolo model to identify the kind of avo. Since we can only predict Hass, maybe I need something to account for this. \n",
    "\n",
    "Next we move these images into a separate checking folder. This is to make the rest of the code sustainable and work across all devices consistently. This will move all the Hass crops into a separate checking folder (or shepards at a later date, may need an if else or something) and then delete the runs folder to avoid having the future cops go into 'predict2' and then having to keep track of the number of predicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c673464b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/7 c:\\Users\\johnl\\Documents\\GitHub\\avoChecker\\Pictures\\1024A\\1024A_0.jpg: 640x512 1 Hass, 14.3ms\n",
      "image 2/7 c:\\Users\\johnl\\Documents\\GitHub\\avoChecker\\Pictures\\1024A\\1024A_1.jpg: 640x512 1 Hass, 10.0ms\n",
      "image 3/7 c:\\Users\\johnl\\Documents\\GitHub\\avoChecker\\Pictures\\1024A\\1024A_2.jpg: 640x512 1 Hass, 13.1ms\n",
      "image 4/7 c:\\Users\\johnl\\Documents\\GitHub\\avoChecker\\Pictures\\1024A\\1024A_3.jpg: 640x512 1 Hass, 13.3ms\n",
      "image 5/7 c:\\Users\\johnl\\Documents\\GitHub\\avoChecker\\Pictures\\1024A\\1024A_4.jpg: 640x512 1 Hass, 12.3ms\n",
      "image 6/7 c:\\Users\\johnl\\Documents\\GitHub\\avoChecker\\Pictures\\1024A\\1024A_5.jpg: 640x512 1 Hass, 9.5ms\n",
      "image 7/7 c:\\Users\\johnl\\Documents\\GitHub\\avoChecker\\Pictures\\1024A\\1024A_6.jpg: 640x512 2 Hasss, 13.1ms\n",
      "Speed: 2.0ms preprocess, 12.2ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Results saved to \u001b[1mruns\\detect\\predict\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model_yolo = YOLO(\"avoID.pt\")\n",
    "\n",
    "model_yolo.predict(\n",
    "        source='Pictures/1024A',\n",
    "        save_crop=True,\n",
    "        save=True,\n",
    "        conf=0.5\n",
    "    )\n",
    "#!yolo predict model=avoID.pt source='Pictures/1020' conf = 0.5 save_crop=True save\n",
    "shutil.move('runs/detect/predict/crops/Hass','checking_folder/') #need to add a line if there are no hass' detected\n",
    "shutil.rmtree('runs/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b22b51",
   "metadata": {},
   "source": [
    "The next steps: \n",
    "1) Define a function to resize the images inputed\n",
    "2) open checking_folder and store the names of each file in a list\n",
    "3) loop through that list and resize each image\n",
    "4) run the image through the algo and save the score\n",
    "5) get an average of the final score as the score for the avos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77550121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.16428"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is to resize the image prior to running it through the model\n",
    "def prepare_input(image_path, target_size=(256, 256)): #adjust target size to match model input.\n",
    "    img = Image.open(image_path).resize(target_size)\n",
    "    img_array = np.array(img) / 255.0  # Normalize pixel values\n",
    "    img_array = np.expand_dims(img_array, axis=0) # Add batch dimension\n",
    "    return img_array\n",
    "\n",
    "avo_pics = os.listdir('checking_folder/')\n",
    "\n",
    "avo_score = [] #empty list to store the avo score\n",
    "\n",
    "#Loop through all the pics in the folder to get the individual predictions for each avo\n",
    "for pic in avo_pics:\n",
    "    avo = prepare_input('checking_folder/' + pic)\n",
    "    avo_score.append(model.predict(avo))\n",
    "\n",
    "shutil.rmtree('checking_folder/') #delete the checking_folder to ensure we can use exactly the same code in the futur\n",
    "\n",
    "avo_score = np.concatenate(avo_score, axis=0 )\n",
    "avo_score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6483431e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\johnl\\AppData\\Local\\Temp\\tmpmfz11kke\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\johnl\\AppData\\Local\\Temp\\tmpmfz11kke\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\johnl\\AppData\\Local\\Temp\\tmpmfz11kke'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name='input_layer')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2254585538384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2254585540880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2254585538960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2254585539152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2254585541264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2254585542416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2254585543184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2254585544528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2254585542992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2254585543376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2254585543760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2254586725968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2254586725392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2254586727504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "model_lite = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb6da23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.164278"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Lite model testing\n",
    "\n",
    "##Ok this works well and is a little faster\n",
    "\n",
    "def prepare_input(image_path, target_size=(256, 256)):\n",
    "    img = Image.open(image_path).resize(target_size)\n",
    "    img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    return img_array\n",
    "\n",
    "model_lite = tf.lite.Interpreter(model_path=\"avo_model_lite.tflite\")\n",
    "model_lite.allocate_tensors()\n",
    "input_details = model_lite.get_input_details()\n",
    "output_details = model_lite.get_output_details()\n",
    "\n",
    "def predict_tflite(input_data):\n",
    "    model_lite.set_tensor(input_details[0]['index'], input_data)\n",
    "    model_lite.invoke()\n",
    "    return model_lite.get_tensor(output_details[0]['index'])\n",
    "\n",
    "avo_pics = os.listdir('checking_folder/')\n",
    "\n",
    "avo_score = [] #empty list to store the avo score\n",
    "\n",
    "#Loop through all the pics in the folder to get the individual predictions for each avo\n",
    "for pic in avo_pics:\n",
    "    avo = prepare_input('checking_folder/' + pic)\n",
    "    avo_score.append(predict_tflite(avo))\n",
    "\n",
    "shutil.rmtree('checking_folder/') #delete the checking_folder to ensure we can use exactly the same code in the futur\n",
    "\n",
    "avo_score = np.concatenate(avo_score, axis=0 )\n",
    "avo_score.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
